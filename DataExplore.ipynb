{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine data and create features\n",
    "\n",
    "- Dataset from https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As U.S. budget fight looms, Republicans flip t...</td>\n",
       "      <td>WASHINGTON (Reuters) - The head of a conservat...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S. military to accept transgender recruits o...</td>\n",
       "      <td>WASHINGTON (Reuters) - Transgender people will...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 29, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior U.S. Republican senator: 'Let Mr. Muell...</td>\n",
       "      <td>WASHINGTON (Reuters) - The special counsel inv...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FBI Russia probe helped by Australian diplomat...</td>\n",
       "      <td>WASHINGTON (Reuters) - Trump campaign adviser ...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 30, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trump wants Postal Service to charge 'much mor...</td>\n",
       "      <td>SEATTLE/WASHINGTON (Reuters) - President Donal...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 29, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>White House, Congress prepare for talks on spe...</td>\n",
       "      <td>WEST PALM BEACH, Fla./WASHINGTON (Reuters) - T...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 29, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Trump says Russia probe will be fair, but time...</td>\n",
       "      <td>WEST PALM BEACH, Fla (Reuters) - President Don...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 29, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Factbox: Trump on Twitter (Dec 29) - Approval ...</td>\n",
       "      <td>The following statements were posted to the ve...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 29, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Trump on Twitter (Dec 28) - Global Warming</td>\n",
       "      <td>The following statements were posted to the ve...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 29, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Alabama official to certify Senator-elect Jone...</td>\n",
       "      <td>WASHINGTON (Reuters) - Alabama Secretary of St...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 28, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  As U.S. budget fight looms, Republicans flip t...   \n",
       "1  U.S. military to accept transgender recruits o...   \n",
       "2  Senior U.S. Republican senator: 'Let Mr. Muell...   \n",
       "3  FBI Russia probe helped by Australian diplomat...   \n",
       "4  Trump wants Postal Service to charge 'much mor...   \n",
       "5  White House, Congress prepare for talks on spe...   \n",
       "6  Trump says Russia probe will be fair, but time...   \n",
       "7  Factbox: Trump on Twitter (Dec 29) - Approval ...   \n",
       "8         Trump on Twitter (Dec 28) - Global Warming   \n",
       "9  Alabama official to certify Senator-elect Jone...   \n",
       "\n",
       "                                                text       subject  \\\n",
       "0  WASHINGTON (Reuters) - The head of a conservat...  politicsNews   \n",
       "1  WASHINGTON (Reuters) - Transgender people will...  politicsNews   \n",
       "2  WASHINGTON (Reuters) - The special counsel inv...  politicsNews   \n",
       "3  WASHINGTON (Reuters) - Trump campaign adviser ...  politicsNews   \n",
       "4  SEATTLE/WASHINGTON (Reuters) - President Donal...  politicsNews   \n",
       "5  WEST PALM BEACH, Fla./WASHINGTON (Reuters) - T...  politicsNews   \n",
       "6  WEST PALM BEACH, Fla (Reuters) - President Don...  politicsNews   \n",
       "7  The following statements were posted to the ve...  politicsNews   \n",
       "8  The following statements were posted to the ve...  politicsNews   \n",
       "9  WASHINGTON (Reuters) - Alabama Secretary of St...  politicsNews   \n",
       "\n",
       "                 date  label  \n",
       "0  December 31, 2017       1  \n",
       "1  December 29, 2017       1  \n",
       "2  December 31, 2017       1  \n",
       "3  December 30, 2017       1  \n",
       "4  December 29, 2017       1  \n",
       "5  December 29, 2017       1  \n",
       "6  December 29, 2017       1  \n",
       "7  December 29, 2017       1  \n",
       "8  December 29, 2017       1  \n",
       "9  December 28, 2017       1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import random as rn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "true_data = pd.read_csv(\"~/FakeNewsProject/data/s_1/True.csv\")\n",
    "false_data = pd.read_csv(\"~/FakeNewsProject/data/s_1/Fake.csv\") \n",
    "\n",
    "combined_df = pd.concat([true_data, false_data])\n",
    "labels_np = np.concatenate([np.ones(len(true_data), dtype=np.int), np.zeros(len(false_data), dtype=np.int)])\n",
    "\n",
    "labels_df = pd.DataFrame(data=labels_np, index=np.arange(0,(len(labels_np))), columns=['label'])\n",
    "combined_df['label'] = labels_df\n",
    "\n",
    "combined_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "politicsNews:\t11272\n",
      "worldnews:\t10145\n",
      "News:\t9050\n",
      "politics:\t6841\n",
      "left-news:\t4459\n",
      "Government News:\t1570\n",
      "US_News:\t783\n",
      "Middle-east:\t778\n",
      "Total Records:\t44898\n"
     ]
    }
   ],
   "source": [
    "# Counting by Subjects \n",
    "for key,count in combined_df.subject.value_counts().iteritems():\n",
    "    print(f\"{key}:\\t{count}\")\n",
    "    \n",
    "# Getting Total Rows\n",
    "print(f\"Total Records:\\t{combined_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44061\n"
     ]
    }
   ],
   "source": [
    "# get rid of outliers\n",
    "filtered_list = [i for i in range(0,len(combined_df)) if len(combined_df.iloc[i]['text'])> 50]\n",
    "filtered_df = combined_df.iloc[filtered_list]\n",
    "print(len(filtered_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/engineer6080/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/engineer6080/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /home/engineer6080/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/engineer6080/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # download stop words\n",
    "    \n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('brown')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from collections import defaultdict\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WASHINGTON (Reuters) - The head of a conservative Republican faction in the U.S. Congress, who voted this month for a huge expansion of the national debt to pay for tax cuts, called himself a “fiscal conservative” on Sunday and urged budget restraint in 2018.\n"
     ]
    }
   ],
   "source": [
    "sample_text = (filtered_df.iloc[[0]]['text'])[0]\n",
    "text_sentences = sent_tokenize(sample_text)\n",
    "print(text_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WASHINGTON (Reuters)  The head of a conservative Republican faction in the US Congress who voted this month for a huge expansion of the national debt to pay for tax cuts called himself a fiscal conservative on Sunday and urged budget restraint in \n"
     ]
    }
   ],
   "source": [
    "# remove punctuation\n",
    "cleaned = re.sub(r'[^(a-zA-Z)\\s]','', text_sentences[0])\n",
    "print(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "washington (reuters)  the head of a conservative republican faction in the us congress who voted this month for a huge expansion of the national debt to pay for tax cuts called himself a fiscal conservative on sunday and urged budget restraint in \n"
     ]
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "sentence_stem = ps.stem(cleaned)\n",
    "print(sentence_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['washington', '(', 'reuters', ')', 'the', 'head', 'of', 'a', 'conservative', 'republican', 'faction', 'in', 'the', 'us', 'congress', 'who', 'voted', 'this', 'month', 'for', 'a', 'huge', 'expansion', 'of', 'the', 'national', 'debt', 'to', 'pay', 'for', 'tax', 'cuts', 'called', 'himself', 'a', 'fiscal', 'conservative', 'on', 'sunday', 'and', 'urged', 'budget', 'restraint', 'in']\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(sentence_stem)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['washington', 'reuters', 'the', 'head', 'of', 'a', 'conservative', 'republican', 'faction', 'in', 'the', 'us', 'congress', 'who', 'voted', 'this', 'month', 'for', 'a', 'huge', 'expansion', 'of', 'the', 'national', 'debt', 'to', 'pay', 'for', 'tax', 'cuts', 'called', 'himself', 'a', 'fiscal', 'conservative', 'on', 'sunday', 'and', 'urged', 'budget', 'restraint', 'in']\n"
     ]
    }
   ],
   "source": [
    "filtered_words = [word.lower() for word in words if word.isalpha()]\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "''' \n",
    "    Takes in sentence as input\n",
    "    ex: text_sentences = sent_tokenize(text_body)\n",
    "'''\n",
    "def filterWords(sentence):\n",
    "    ps = PorterStemmer()\n",
    "    # remove punctuation\n",
    "    cleaned = re.sub(r'[^(a-zA-Z)\\s]','', sentence)\n",
    "    sentence_stem = ps.stem(cleaned)\n",
    "    words = word_tokenize(sentence_stem)\n",
    "    # remove brackets, etc.\n",
    "    filtered_words = [word.lower() for word in words if word.isalpha()]\n",
    "    return filtered_words\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['washington', 'reuters', 'the', 'head', 'of', 'a', 'conservative', 'republican', 'faction', 'in', 'the', 'us', 'congress', 'who', 'voted', 'this', 'month', 'for', 'a', 'huge', 'expansion', 'of', 'the', 'national', 'debt', 'to', 'pay', 'for', 'tax', 'cuts', 'called', 'himself', 'a', 'fiscal', 'conservative', 'on', 'sunday', 'and', 'urged', 'budget', 'restraint', 'in']\n",
      "\n",
      "CPU times: user 306 µs, sys: 7 µs, total: 313 µs\n",
      "Wall time: 301 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "test_words = filterWords(text_sentences[0])\n",
    "print(test_words, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Takes in sentence\n",
    "    Output ambiguity score \n",
    "'''\n",
    "def ambiguity(sentence):\n",
    "    ambiguousCount = 0\n",
    "    for word in sentence:\n",
    "        ambiguousCount += len(wn.synsets(word))\n",
    "        \n",
    "    return ambiguousCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261\n",
      "\n",
      "CPU times: user 988 ms, sys: 24.1 ms, total: 1.01 s\n",
      "Wall time: 1.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ambCount = ambiguity(test_words)\n",
    "\n",
    "print(ambCount, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    input sentence\n",
    "    out: % unique words\n",
    "'''\n",
    "\n",
    "def lexical_diversity(sentence):\n",
    "        return len(set(sentence)) / len(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.95238095238095\n",
      "\n",
      "CPU times: user 0 ns, sys: 74 µs, total: 74 µs\n",
      "Wall time: 56.7 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "lexCount = lexical_diversity(test_words)\n",
    "\n",
    "print(lexCount*100, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WASHINGTON (Reuters) - The head of a conservative Republican faction in the U.S. Congress,\n",
      "[0.137, 0.811, 0.052, -0.4215]\n",
      "\n",
      "\n",
      "In keeping with a sharp pivot under way among Republicans, U.S. Representative Mark Meadow\n",
      "[0.108, 0.892, 0.0, -0.4588]\n",
      "\n",
      "\n",
      "When they return from the holidays on Wednesday, lawmakers will begin trying to pass a fed\n",
      "[0.052, 0.896, 0.052, 0.0]\n",
      "\n",
      "\n",
      "President Donald Trump and his Republicans want a big budget increase in military spending\n",
      "[0.0, 0.808, 0.192, 0.6808]\n",
      "\n",
      "\n",
      "“The (Trump) administration has already been willing to say: ‘We’re going to increase non-\n",
      "[0.0, 0.733, 0.267, 0.9062]\n",
      "\n",
      "\n",
      "“Now, Democrats are saying that’s not enough, we need to give the government a pay raise o\n",
      "[0.069, 0.931, 0.0, -0.1027]\n",
      "\n",
      "\n",
      "For a fiscal conservative, I don’t see where the rationale is.\n",
      "[0.0, 1.0, 0.0, 0.0]\n",
      "\n",
      "\n",
      "...\n",
      "[0.0, 1.0, 0.0, 0.0]\n",
      "\n",
      "\n",
      "Eventually you run out of other people’s money,” he said.\n",
      "[0.0, 1.0, 0.0, 0.0]\n",
      "\n",
      "\n",
      "Meadows was among Republicans who voted in late December for their party’s debt-financed t\n",
      "[0.126, 0.874, 0.0, -0.6369]\n",
      "\n",
      "\n",
      "“It’s interesting to hear Mark talk about fiscal responsibility,” Democratic U.S. Represen\n",
      "[0.0, 0.856, 0.144, 0.4019]\n",
      "\n",
      "\n",
      "Crowley said the Republican tax bill would require the  United States to borrow $1.5 trill\n",
      "[0.06, 0.765, 0.175, 0.6369]\n",
      "\n",
      "\n",
      "“This is one of the least ... fiscally responsible bills we’ve ever seen passed in the his\n",
      "[0.0, 0.901, 0.099, 0.3182]\n",
      "\n",
      "\n",
      "I think we’re going to be paying for this for many, many years to come,” Crowley said.\n",
      "[0.0, 1.0, 0.0, 0.0]\n",
      "\n",
      "\n",
      "Republicans insist the tax package, the biggest U.S. tax overhaul in more than 30 years,  \n",
      "[0.0, 0.791, 0.209, 0.6486]\n",
      "\n",
      "\n",
      "House Speaker Paul Ryan, who also supported the tax bill, recently went further than Meado\n",
      "[0.0, 0.783, 0.217, 0.8126]\n",
      "\n",
      "\n",
      "In Republican parlance, “entitlement” programs mean food stamps, housing assistance, Medic\n",
      "[0.142, 0.751, 0.106, -0.34]\n",
      "\n",
      "\n",
      "Democrats seized on Ryan’s early December remarks, saying they showed Republicans would tr\n",
      "[0.13, 0.87, 0.0, -0.3818]\n",
      "\n",
      "\n",
      "But the goals of House Republicans may have to take a back seat to the Senate, where the v\n",
      "[0.0, 0.965, 0.035, 0.0258]\n",
      "\n",
      "\n",
      "Democrats will use their leverage in the Senate, which Republicans narrowly control, to de\n",
      "[0.0, 1.0, 0.0, 0.0]\n",
      "\n",
      "\n",
      "Trump in September put a March 2018 expiration date on the Deferred Action for Childhood A\n",
      "[0.0, 0.929, 0.071, 0.3182]\n",
      "\n",
      "\n",
      "The president has said in recent Twitter messages he wants funding for his proposed Mexica\n",
      "[0.0, 0.846, 0.154, 0.6249]\n",
      "\n",
      "\n",
      "Representative Debbie Dingell told CBS she did not favor linking that issue to other polic\n",
      "[0.106, 0.894, 0.0, -0.3089]\n",
      "\n",
      "\n",
      "“We need to do DACA clean,” she said.\n",
      "[0.0, 1.0, 0.0, 0.0]\n",
      "\n",
      "\n",
      "On Wednesday, Trump aides will meet with congressional leaders to discuss those issues.\n",
      "[0.0, 1.0, 0.0, 0.0]\n",
      "\n",
      "\n",
      "That will be followed by a weekend of strategy sessions for Trump and Republican leaders o\n",
      "[0.0, 1.0, 0.0, 0.0]\n",
      "\n",
      "\n",
      "Trump was also scheduled to meet on Sunday with Florida Republican Governor Rick Scott, wh\n",
      "[0.138, 0.862, 0.0, -0.4391]\n",
      "\n",
      "\n",
      "The House has passed an $81 billion aid package after hurricanes in Florida, Texas and Pue\n",
      "[0.0, 1.0, 0.0, 0.0]\n",
      "\n",
      "\n",
      "The package far exceeded the $44 billion requested by the Trump administration.\n",
      "[0.0, 1.0, 0.0, 0.0]\n",
      "\n",
      "\n",
      "The Senate has not yet voted on the aid.\n",
      "[0.0, 1.0, 0.0, 0.0]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "def analyzeSentiment(sentence):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    ss = sid.polarity_scores(sentence) # test_words\n",
    "    \n",
    "    return [ss['neg'], ss['neu'], ss['pos'], ss['compound']]\n",
    "\n",
    "for s in text_sentences:\n",
    "    print(s[0:90])\n",
    "    ss = analyzeSentiment(s)\n",
    "    print(ss)\n",
    "    '''\n",
    "    for k in sorted(ss):\n",
    "        print('{0}: {1}, '.format(k, ss[k]), end='')\n",
    "    '''\n",
    "\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title size: 11 Article size: 827\n",
      "Vectorizer shape: (827, 369)\n",
      "0.002463661000246366\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from pandas.core.common import flatten\n",
    "\n",
    "article_num = 4\n",
    "article_words = list(flatten([filterWords(s) for s in sent_tokenize(filtered_df.iloc[article_num]['text'])]))\n",
    "title_words = filterWords((filtered_df.iloc[article_num]['title']))\n",
    "    \n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(article_words)\n",
    "#print(vectorizer.get_feature_names())\n",
    "print(\"Title size:\", len(title_words), \"Article size:\", len(article_words))\n",
    "print(\"Vectorizer shape:\", X.shape)\n",
    "\n",
    "out_arr = vectorizer.transform(title_words).toarray()\n",
    "print(np.average(out_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidVectorizer(sentence_list, title):\n",
    "    article_words = list(flatten(sentence_list))\n",
    "    title_words = filterWords(title)\n",
    "    \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(article_words)\n",
    "    out_arr = vectorizer.transform(title_words).toarray()\n",
    "    \n",
    "    return np.average(out_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44061"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processors: 16\n",
      "2753 13\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from multiprocessing import Pool, Process, Manager, Lock\n",
    "\n",
    "PROCS = os.cpu_count()\n",
    "print(\"Processors:\", PROCS)\n",
    "\n",
    "manager = Manager()\n",
    "pLock = Lock()\n",
    "shared_dict = manager.dict()\n",
    "\n",
    "quant = len(filtered_df)\n",
    "#quant = 5000\n",
    "\n",
    "sliceSize = int(quant/(PROCS))\n",
    "sliceRemain = quant%(PROCS)\n",
    "\n",
    "print(sliceSize, sliceRemain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preProcessData(myid, rng, filtered_df, shared_dict):\n",
    "    \n",
    "    pLock.acquire()\n",
    "    print(\"ID:\", myid, \"START:\", rng.start, \"STOP:\", rng.stop)\n",
    "    pLock.release()\n",
    "\n",
    "    output_list = []\n",
    "    err_list = []\n",
    "    sentence_list = []\n",
    "    # title, text, subject, date \n",
    "\n",
    "    for i in rng:\n",
    "        text_body = (filtered_df.iloc[i]['text'])\n",
    "        text_sentences = sent_tokenize(text_body)\n",
    "\n",
    "        feature_dict = {}\n",
    "        \n",
    "        amb_list = []\n",
    "        lex_list = []\n",
    "        sentiment_list = []\n",
    "        \n",
    "        try:\n",
    "\n",
    "            # df columns [ambiguity, lexical_diversity, sentiment]\n",
    "            words_list = []\n",
    "            for s in text_sentences:\n",
    "                #if(len(s) > 5):\n",
    "                cleaned_words = filterWords(s)\n",
    "                words_list.append(cleaned_words)\n",
    "\n",
    "                amb_list.append(ambiguity(cleaned_words))\n",
    "                lex_list.append(lexical_diversity(cleaned_words))\n",
    "                sentiment_list.append(analyzeSentiment(s))\n",
    "\n",
    "        \n",
    "            sentence_list.append(words_list)\n",
    "            \n",
    "            feature_dict['tfid'] = tfidVectorizer(words_list, filtered_df.iloc[i]['title'])\n",
    "    \n",
    "            feature_dict['id'] = i\n",
    "            feature_dict['ambg'] = np.mean(amb_list)\n",
    "            feature_dict['lex_div'] = np.mean(lex_list)\n",
    "            \n",
    "            mean_sentiment = np.mean(np.array(sentiment_list), axis=0)\n",
    "            feature_dict['neg'] = mean_sentiment[0]\n",
    "            feature_dict['neu'] = mean_sentiment[1]\n",
    "            feature_dict['pos'] = mean_sentiment[2]\n",
    "            feature_dict['compound'] = mean_sentiment[3]\n",
    "            feature_dict['label'] =  (filtered_df.iloc[i]['label'])\n",
    "            \n",
    "        except Exception as e:\n",
    "            \n",
    "            feature_dict['id'] = i\n",
    "            feature_dict['ambg'] = 0\n",
    "            feature_dict['lex_div'] = 0\n",
    "            feature_dict['neg'] = 0\n",
    "            feature_dict['neu'] = 0\n",
    "            feature_dict['pos'] = 0\n",
    "            feature_dict['compound'] = 0\n",
    "            feature_dict['tfid'] = 0 \n",
    "            feature_dict['label'] = -1\n",
    "            \n",
    "            \n",
    "            err_str = \"id: \" + str(myid) + \"iter: \" + str(i) + \"err: \" + str(e)\n",
    "            err_list.append(err_str)\n",
    "        \n",
    "        output_list.append(feature_dict)\n",
    "    \n",
    "    pLock.acquire()\n",
    "    shared_dict[myid] = output_list\n",
    "    shared_dict[(str(myid) + \"_err\")] = err_list\n",
    "    shared_dict[(str(myid) + \"_words\")] = sentence_list\n",
    "    print(\"ID:\", myid, \"FIN\")\n",
    "    pLock.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Debugging\n",
    "    \n",
    "    shared_dict = {}\n",
    "    preProcessData(0, range(0,1), filtered_df, shared_dict)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 0 START: 0 STOP: 2753\n",
      "ID: 1  START:2753 STOP: 5506\n",
      "ID:  2START: 5506 STOP: 8259\n",
      "ID: 3 START: 8259 STOP: 11012\n",
      "ID: 4 START: 11012 STOP: 13765\n",
      "ID: 5 START: 13765 STOP: 16518\n",
      "ID: 6 START: 16518 STOP: 19271\n",
      "ID: 7 START: 19271 STOP: 22024\n",
      "ID: 8 START: 22024 STOP: 24777\n",
      "ID: 9 START: 24777 STOP: 27530\n",
      "ID: 10 START: 27530 STOP: 30283\n",
      "ID: 11 START: 30283 STOP: 33036\n",
      "ID: 12 START: 33036 STOP: 35789\n",
      "ID: 13 START: 35789 STOP: 38542\n",
      "ID: 14 START: 38542 STOP: 41295\n",
      "ID: 15 START: 41295 STOP: 44061\n",
      "ID: 12 FIN\n",
      "ID: 14 FIN\n",
      "ID: 5 FIN\n",
      "ID: 4 FIN\n",
      "ID: 7 FIN\n",
      "ID: 11 FIN\n",
      "ID: 13 FIN\n",
      "ID: 6 FIN\n",
      "ID: 1 FIN\n",
      "ID: 3 FIN\n",
      "ID: 10 FIN\n",
      "ID: 0 FIN\n",
      "ID: 9 FIN\n",
      "ID: 2 FIN\n",
      "ID: 8 FIN\n",
      "ID: 15 FIN\n",
      "CPU times: user 78.2 ms, sys: 142 ms, total: 220 ms\n",
      "Wall time: 4min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "processes = []\n",
    "\n",
    "for i in range(0, PROCS):\n",
    "    \n",
    "    rng = range( (i*sliceSize), ((i+1)*sliceSize)  ) \n",
    "    \n",
    "    if(i == PROCS-1):\n",
    "        rng = range( ((i)*sliceSize), ((i+1)*sliceSize)+sliceRemain )\n",
    "        \n",
    "    p = Process(target=preProcessData, args=[ i, rng, filtered_df, shared_dict ])\n",
    "    processes.append(p)\n",
    "    p.start()\n",
    "    \n",
    "    \n",
    "# Wait for finish\n",
    "for p in processes:\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1728"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(shared_dict['0_words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2753 2753 2753 2753 2753 2753 2753 2753 2753 2753 2753 2753 2753 2753 2753 2766 "
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Combine\n",
    "'''\n",
    "\n",
    "featureList = []\n",
    "\n",
    "for pid in range(0, PROCS):\n",
    "    print(len(shared_dict[pid]), end = \" \")\n",
    "    for s_dict in shared_dict[pid]:\n",
    "        featureList.append(s_dict)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num sentences 1728 num sentences 1512 num sentences 1543 num sentences 1473 num sentences 1476 num sentences 1438 num sentences 1417 num sentences 1330 num sentences 1429 num sentences 1477 num sentences 1363 num sentences 1376 num sentences 1368 num sentences 1382 num sentences 1430 num sentences 1372 "
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Combine processed words\n",
    "'''\n",
    "\n",
    "paraList = []\n",
    "\n",
    "for pid in range(0, PROCS):\n",
    "    idx = (str(pid)+\"_words\")\n",
    "    print(\"num sentences\", len(shared_dict[idx]), end = \" \")\n",
    "    for p in shared_dict[idx]:\n",
    "        paraList.append(p)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23114 44061\n"
     ]
    }
   ],
   "source": [
    "print(len(paraList), len(featureList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('paraList.pkl', 'wb') as f:\n",
    "    pickle.dump(paraList, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>ambg</th>\n",
       "      <th>lex_div</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "      <th>tfid</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>411</td>\n",
       "      <td>129.500000</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.942500</td>\n",
       "      <td>0.057500</td>\n",
       "      <td>0.255300</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>502</td>\n",
       "      <td>196.000000</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040909</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>506</td>\n",
       "      <td>281.500000</td>\n",
       "      <td>0.821870</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.922500</td>\n",
       "      <td>0.077500</td>\n",
       "      <td>0.431600</td>\n",
       "      <td>0.011544</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>530</td>\n",
       "      <td>88.166667</td>\n",
       "      <td>0.953922</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.053333</td>\n",
       "      <td>0.101950</td>\n",
       "      <td>0.012000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>541</td>\n",
       "      <td>169.500000</td>\n",
       "      <td>0.910401</td>\n",
       "      <td>0.079500</td>\n",
       "      <td>0.876500</td>\n",
       "      <td>0.044000</td>\n",
       "      <td>-0.088950</td>\n",
       "      <td>0.015957</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>543</td>\n",
       "      <td>85.666667</td>\n",
       "      <td>0.956699</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019027</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>548</td>\n",
       "      <td>173.666667</td>\n",
       "      <td>0.924731</td>\n",
       "      <td>0.026333</td>\n",
       "      <td>0.973667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.120400</td>\n",
       "      <td>0.017296</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>585</td>\n",
       "      <td>100.750000</td>\n",
       "      <td>0.906609</td>\n",
       "      <td>0.138500</td>\n",
       "      <td>0.846500</td>\n",
       "      <td>0.015000</td>\n",
       "      <td>-0.133200</td>\n",
       "      <td>0.017825</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>629</td>\n",
       "      <td>226.500000</td>\n",
       "      <td>0.843791</td>\n",
       "      <td>0.043500</td>\n",
       "      <td>0.956500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.263350</td>\n",
       "      <td>0.014550</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>654</th>\n",
       "      <td>654</td>\n",
       "      <td>127.000000</td>\n",
       "      <td>0.974138</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.976250</td>\n",
       "      <td>0.023750</td>\n",
       "      <td>0.056575</td>\n",
       "      <td>0.011785</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id        ambg   lex_div       neg       neu       pos  compound  \\\n",
       "411  411  129.500000  0.958333  0.000000  0.942500  0.057500  0.255300   \n",
       "502  502  196.000000  0.814815  0.000000  1.000000  0.000000  0.000000   \n",
       "506  506  281.500000  0.821870  0.000000  0.922500  0.077500  0.431600   \n",
       "530  530   88.166667  0.953922  0.000000  0.946667  0.053333  0.101950   \n",
       "541  541  169.500000  0.910401  0.079500  0.876500  0.044000 -0.088950   \n",
       "543  543   85.666667  0.956699  0.000000  1.000000  0.000000  0.000000   \n",
       "548  548  173.666667  0.924731  0.026333  0.973667  0.000000 -0.120400   \n",
       "585  585  100.750000  0.906609  0.138500  0.846500  0.015000 -0.133200   \n",
       "629  629  226.500000  0.843791  0.043500  0.956500  0.000000 -0.263350   \n",
       "654  654  127.000000  0.974138  0.000000  0.976250  0.023750  0.056575   \n",
       "\n",
       "         tfid  label  \n",
       "411  0.025000      1  \n",
       "502  0.040909      1  \n",
       "506  0.011544      1  \n",
       "530  0.012000      1  \n",
       "541  0.015957      1  \n",
       "543  0.019027      1  \n",
       "548  0.017296      1  \n",
       "585  0.017825      1  \n",
       "629  0.014550      1  \n",
       "654  0.011785      1  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df = pd.DataFrame(featureList)\n",
    "final_df = final_df[final_df.label != -1]\n",
    "\n",
    "final_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the dataframe \n",
    "final_df.to_csv('feature_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the raw combined dataframe \n",
    "filtered_df.to_csv('combined_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "sinsat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
